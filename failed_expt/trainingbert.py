# -*- coding: utf-8 -*-
"""tarining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CBwM7r40vdsI4ASpabbZtI8U5P6nJwIT
"""

# Commented out IPython magic to ensure Python compatibility.
# 
# 
# %%capture
# !pip install transformers
# 
# 
# import torch
# import numpy as np
# import pandas as pd
# 
# from sklearn.model_selection import train_test_split
# from torch.utils.data import Dataset, DataLoader
# from transformers import AutoTokenizer, BertModel, BertForSequenceClassification
# 
# 
# # Set up the GPU.
# device = 'cuda' if torch.cuda.is_available() else 'cpu'
# device
# 
#

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

url = '/content/reviews_v1_hiring_task.csv'
df = pd.read_csv(url)
df = df[['reviews.rating', 'reviews.text']]
df

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

!export TORCH_USE_CUDA_DSA=1

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels = len(df['reviews.rating'].unique()), # number of unique labels for our multi-class classification problem
    output_attentions = False,
    output_hidden_states = False,
)
model.to(device)

class ReviewsDataset(Dataset):
    def __init__(self, df, max_length=32):
        self.df = df
        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        self.max_length = max_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # input=review, label=stars
        review = self.df.loc[idx, 'reviews.text']
        # labels are 0-indexed
        label = int(self.df.loc[idx, 'reviews.rating'])

        encoded = self.tokenizer(
            review,                      # review to encode
            add_special_tokens=True,
            max_length=self.max_length,  # Truncate all segments to max_length
            padding='max_length',        # pad all reviews with the [PAD] token to the max_length
            return_attention_mask=True,  # Construct attention masks.
            truncation=True
        )

        input_ids = encoded['input_ids']
        attn_mask = encoded['attention_mask']

        return {
            'input_ids': torch.tensor(input_ids),
            'attn_mask': torch.tensor(attn_mask),
            'label': torch.tensor(label)
        }

train_dataset, test_dataset = train_test_split(df, test_size=0.2, random_state=1)

MAX_LEN = 32
TEST_SIZE = 0.2
VAL_SIZE = 0.125
TRAIN_BATCH_SIZE = 16
TEST_BATCH_SIZE = 16

CHECKPOINT_FILE = 'checkpoint1.dat'
CHECKPOINT_FOLDER = 'Checkpoint1'
EPOCHS = 4
LEARNING_RATE = 2e-05
PROJECT_FOLDER = '/content/drive/My Drive/BERT project/'
MODEL_FOLDER = 'Model_V3'
SAVE_EVERY = 10
NUM_WORKERS = 2

train_dataset, test_dataset = train_test_split(df, test_size=TEST_SIZE, random_state=1)
train_dataset, val_dataset = train_test_split(train_dataset, test_size=VAL_SIZE, random_state=1)

train_dataset = train_dataset.reset_index(drop=True)
val_dataset = val_dataset.reset_index(drop=True)
test_dataset = test_dataset.reset_index(drop=True)

train_set = ReviewsDataset(train_dataset, MAX_LEN)
val_set = ReviewsDataset(val_dataset, MAX_LEN)
test_set = ReviewsDataset(test_dataset, MAX_LEN)

print("# of samples in train set: {}".format(len(train_set)))
print("# of samples in val set: {}".format(len(val_set)))
print("# of samples in test set: {}".format(len(test_set)))

train_params = {
                'batch_size': TRAIN_BATCH_SIZE,
                'shuffle': True,

                }
val_params = train_params

test_params = {
                'batch_size': TEST_BATCH_SIZE,
                'shuffle': False,

              }

train_loader = DataLoader(train_set, **train_params)
val_loader = DataLoader(val_set, **val_params)
test_loader = DataLoader(test_set, **test_params)

# For weighted Cross Entropy Loss
# Penalize errors higher if they come from a class with lower frequency
r_groups = df.groupby('reviews.rating')
r_distribution = []
for i in range(len(df['reviews.rating'].unique())):
    r_distribution.append(len(r_groups.groups[i+1])/len(df))

r_distribution = torch.tensor(r_distribution, dtype=torch.float32)

# V3
weights = 1.0 / r_distribution
weights = weights / weights.sum()

# V4
# weights = 1.0 - star_distribution

print('{:<20}: {}'.format('rating distribution', r_distribution.tolist()))
print('{:<20}: {}'.format('Weights', weights.tolist()))

# Define the optimizer
# Ensure weights sum to 1
weights = weights / weights.sum()

# Check for NaN or Inf and replace with 0 if found
weights[torch.isnan(weights)] = 0
weights[torch.isinf(weights)] = 0

loss_function = torch.nn.CrossEntropyLoss(weight=weights.to(device), reduction='mean')
optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)

# Print weights to verify
print(weights)

# Define the optimizer
# Ensure weights sum to 1
weights = weights / weights.sum()

# Check for NaN or Inf and replace with 0 if found
weights[torch.isnan(weights)] = 0
weights[torch.isinf(weights)] = 0

# Clamp weights to be non-negative
weights = torch.clamp(weights, min=0) # Ensure all weights are non-negative

# Print weights to verify
print(weights)

# Define the optimizer
loss_function = torch.nn.CrossEntropyLoss(weight=weights.to(device), reduction='mean')
optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)

# Define the accuracy function
def calculate_accuracy(big_idx, targets):
    n_correct = (big_idx==targets).sum().item()
    return n_correct

# For validation
def validate(model, data_loader):
    model.eval()
    n_correct = 0
    nb_test_steps = 0
    nb_test_examples = 0
    test_loss = 0
    y_pred = []
    y_true = []

    with torch.no_grad():
        for _, data in enumerate(data_loader, 0):
            input_ids = data['input_ids'].to(device)
            mask = data['attn_mask'].to(device)
            labels = data['label'].to(device)

            outputs = model(input_ids, mask)
            loss = loss_function(outputs[0], labels)
            test_loss += loss.item()

            # gets labels with highest probabilities and their corresponding indices
            big_val, big_idx = torch.max(outputs[0].data, dim=1)
            n_correct += calculate_accuracy(big_idx, labels)

            preds = (big_idx + 1).cpu().tolist()

            y_pred.extend(preds)


            nb_test_steps += 1
            nb_test_examples += labels.size(0)

    epoch_loss = test_loss/nb_test_steps
    epoch_accu = (n_correct*100)/nb_test_examples
    print(f"Validation Loss: {epoch_loss}")
    print(f"Validation Accuracy: {epoch_accu}\n")

    return y_true, y_pred, epoch_accu

import os
def train(epoch):
    # number of batches run by model
    nb_tr_steps = 0
    # number of training examples run by model
    nb_tr_examples = 0
    # number of examples classified correctly by model
    n_correct = 0
    tr_loss = 0
    model.train()

    for batch, data in enumerate(train_loader):
        input_ids = data['input_ids'].to(device)
        mask = data['attn_mask'].to(device)
        labels = data['label'].to(device)

        outputs = model(input_ids, mask)
        loss = loss_function(outputs[0], labels)
        tr_loss += loss.item()

        # gets labels with highest probabilities and their corresponding indices
        big_val, big_idx = torch.max(outputs[0].data, dim=1)
        n_correct += calculate_accuracy(big_idx, labels)

        nb_tr_steps += 1
        nb_tr_examples+=labels.size(0)

        if batch % SAVE_EVERY == 0:
            loss_step = tr_loss/nb_tr_steps
            accu_step = (n_correct*100)/nb_tr_examples
            print("Batch {} of epoch {} complete.".format(batch, epoch+1))
            print(f"Training Loss: {loss_step}   Training Accuracy: {accu_step}")

            if not os.path.exists(CHECKPOINT_FOLDER):
              os.makedirs(CHECKPOINT_FOLDER)

            # Since a single epoch could take well over hours, we regularly save the model even during evaluation of training accuracy.
            torch.save(model.state_dict(), os.path.join(PROJECT_FOLDER, CHECKPOINT_FOLDER, CHECKPOINT_FILE))
            print("Saving checkpoint at", os.path.join(PROJECT_FOLDER, CHECKPOINT_FOLDER, CHECKPOINT_FILE))

        optimizer.zero_grad()
        loss.backward()
        # When using GPU
        optimizer.step()

    print('\n*****\n')
    print(f'The Total Accuracy for Epoch {epoch+1}: {(n_correct*100)/nb_tr_examples}')
    epoch_loss = tr_loss/nb_tr_steps
    epoch_accu = (n_correct*100)/nb_tr_examples
    print(f"Training Loss: {epoch_loss}")
    print(f"Training Accuracy: {epoch_accu}\n")

    # Evaluate model after training it on this epoch
    validate(model, val_loader)

    torch.save(model.state_dict(), os.path.join(PROJECT_FOLDER, CHECKPOINT_FOLDER, CHECKPOINT_FILE))
    model.save_pretrained(os.path.join(PROJECT_FOLDER, MODEL_FOLDER, str(epoch+1)))
    print("Saving checkpoint at ", os.path.join(PROJECT_FOLDER, CHECKPOINT_FOLDER, CHECKPOINT_FILE))
    print("Saving model at ", os.path.join(PROJECT_FOLDER, MODEL_FOLDER, str(epoch+1)), '\n\n================================================\n')

    return

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

import os

# Enable device-side assertions for PyTorch
os.environ['TORCH_USE_CUDA_DSA'] = '1'

import torch

for epoch in range(EPOCHS):
    train(epoch)